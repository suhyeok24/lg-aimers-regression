# ğŸš˜ ììœ¨ì£¼í–‰ ì„¼ì„œì˜ ì•ˆí…Œë‚˜ ì„±ëŠ¥ ì˜ˆì¸¡ AI ê²½ì§„ëŒ€íšŒ ğŸš˜

Regressionì„ í†µí•œ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ê·¹ëŒ€í™”í•´ë³´ì!

<br/><br/>

## Team

**Name** : ì•ˆì¥ìµœ

**Members** : ê³ ë ¤ëŒ€í•™êµ ì‚°ì—…ê²½ì˜ê³µí•™ë¶€ ì•ˆì˜ì§€, ì¥ìˆ˜í˜, ìµœëŒ€ì›

<br/><br/>

## Project Descriptions

**Link** : [ììœ¨ì£¼í–‰ ì„¼ì„œì˜ ì•ˆí…Œë‚˜ ì„±ëŠ¥ ì˜ˆì¸¡ AI ê²½ì§„ëŒ€íšŒ (DACON)](https://dacon.io/competitions/official/235927/overview/description)


**[ëª©í‘œ]**  
ê³µì • ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ Radar ì„¼ì„œì˜ ì•ˆí…Œë‚˜ ì„±ëŠ¥ ì˜ˆì¸¡ì„ ìœ„í•œ AI ëª¨ë¸ ê°œë°œ 


**[ë°°ê²½]**  
RadarëŠ” ììœ¨ì£¼í–‰ ì°¨ì— ìˆì–´ ì°¨ëŸ‰ê³¼ì˜ ê±°ë¦¬, ìƒëŒ€ ì†ë„, ë°©í–¥ ë“±ì„ ì¸¡ì •í•´ì£¼ëŠ” í•„ìˆ˜ì ì¸ ì„¼ì„œ ë¶€í’ˆ <br/>
LGì—ì„œëŠ” ì œí’ˆì˜ ì„±ëŠ¥ í‰ê°€ ê³µì •ì—ì„œ ì–‘í’ˆê³¼ ë¶ˆëŸ‰ì„ ì„ ë³„ ì¤‘. <br/>
AI ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ê³µì • ë°ì´í„°ì™€ ì œí’ˆ ì„±ëŠ¥ê°„ ìƒê´€ ë¶„ì„ì„ í†µí•´ ì œí’ˆì˜ ë¶ˆëŸ‰ì„ ì˜ˆì¸¡/ë¶„ì„í•˜ê³ , 
ìˆ˜ìœ¨ì„ ê·¹ëŒ€í™”í•˜ì—¬ ë¶ˆëŸ‰ìœ¼ë¡œ ì¸í•œ ì œí’ˆ íê¸° ë¹„ìš©ì„ ê°ì¶•ì‹œí‚¤ëŠ” ê²ƒì´ ëª©í‘œ.


**[ì£¼ìµœ]**  
LG AI Research 

**[í‰ê°€ ì‚°ì‹]**  
Normalized RMSE (NRMSE)
> def lg_nrmse(gt, preds):
    # ê° Y Featureë³„ NRMSE ì´í•©
    # Y_01 ~ Y_08 ê¹Œì§€ 20% ê°€ì¤‘ì¹˜ ë¶€ì—¬
    all_nrmse = []
    for idx in range(1,15): # ignore 'ID'
        rmse = metrics.mean_squared_error(gt[:,idx], preds[:,idx], squared=False)
        nrmse = rmse/np.mean(np.abs(gt[:,idx]))
        all_nrmse.append(nrmse)
    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:14])
    return score

<br/><br/>

## ğŸ‘£ Score(Public)
RRDB+ : 23.40812(38th)

<br/><br/>

## ğŸŒ Environment
Colab Pro+  
GPU: A100-SXM4-40GB * 1(Main) , Tesla T4*1(Sub)

<br/><br/>

## ğŸ”¥ Competition Strategies

**1. Patches(for data augmentation)**  
- Train patches : original 1640 images â†’ 26240(1640*16) patches (X4 downsampling, non-overlapping)  
- Test patches: original 18 images â†’ 882(18*49) patches (X4 downsampling, overlapping(to remove border artifacts)) 

<br/>

**2. Data Transform**  
Non-destructive transformations (not to add or lose the information)
- Flip  
- Transpose  
- RandomRotate  
- ShiftScaleRotate  

<br/>

**3. Training Methods**
- EarlyStopping  
  > To prevent overfitting  
  > If validation loss does not improve after given patience(=2), training is earlystopped  
- Fine-tuning with pre-trained model  
  > pretrained model : [RRDB_PSNR_x4.pth](https://github.com/xinntao/ESRGAN/tree/master/models)(the PSNR-oriented model withÂ high PSNR performance)  
  > Retraining entire model : Judging that the similarity between DF2K dataset(pretrained model) and our training datset is small  

<br/>

**4. Loss Function**
- L1 loss + L2 loss (2:1)
  > L2 loss : PSNR is based on MSE  
  > L1 loss: For better convergence [https://arxiv.org/pdf/1707.02921v1.pdf](https://arxiv.org/pdf/1707.02921v1.pdf)

<br/>

**5. Learning Scheduler, Optimizer**
- StepLR  
  > step_size = 3, gamma = 0.5  
  > Decays the learning rate of each parameter in half once per 3 epochs 
- Adam  

<br/>

**6. Post Processing**
- Geometric Self-Ensemble [https://arxiv.org/pdf/1707.02921v1.pdf](https://arxiv.org/pdf/1707.02921v1.pdf)
  > <img width="751" alt="KakaoTalk_Photo_2023-01-04-11-58-49" src="https://user-images.githubusercontent.com/55012723/210476203-015eac00-d0e0-4d10-8eb5-a772a9910097.png">


<br/><br/> 

## Main configuration & Hyperparameters
'''
1. Manuel_seed : 42  

2. Model :  
   > num_feat : 64 , Channel number of intermediate features.  
   > growth_channel: 32 , Channels for each growth(dense connection).  
   > num_block: 23 , number of RRDB blocks.  

3. Dataloader :  
   > train_batch_size : 4  
   > test_batch_size: 1  
   > num_workers: 4   

4. Train :  
   > epochs: 7  
   > optim_g: {type: Adam, lr: 1e-4, betas: [0.9, 0.99]}  

'''

<br/><br/>

## Code Descriptions
1. DACON_AISR_TRIAL
- EDSR, SRGAN, SWINIR


2. DACON_AISR_BEST
- RRDB, RRDB+(Self-ensemble)

